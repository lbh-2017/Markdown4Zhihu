# 【论文阅读】基于强化学习的网络安全防护策略

本篇文章将介绍一篇针对网络安全问题，运用强化学习方法寻找最优的网络防御策略。

### Finding Effective Security Strategies through Reinforcement Learning and Self-Play
- **前言**  
通过强化学习和 **自对弈(self-play)** 寻找有效的安全策略
   - **主要思想**  
提出一种针对入侵防御案例的安全策略自动查找方法，将攻击者和防御者之间的交互建模为一个马尔可夫博弈过程，让攻击和防御过程在没有人干预的情况下通过强化学习和自对弈进行
   - **创新点**  
建立了安全策略的自对弈模型，并解决了强化学习一直存在的问题：1、利用建立模型的结构来减小动作空间的额大小，将动作分解为两个动作，首先选择哪个节点进行攻击或防御，接着选择对该点采用什么攻击或防御策略，将动作空间 **N×(m+1)** 变成了 **N+(m+1)** 2、避免过拟合情况，在训练期间从策略池中抽取对手策略，增加了策略的多样性
   - **存在的问题**  
与所有自对弈问题一样，存在 **训练难以收敛** 的情况，在基础防御较弱时结果容易发生震荡，震荡表明防御者对对手策略的变化做出了应对，并且出现了过拟合情况。  

```
🔥 这篇文章主要贡献是将攻击者和防御者之间的互动建模为一个马尔可夫博弈过程，以及为减小计算量和增加算法的收敛性使用的两个小trick，创新点没有那么突出，但对网络攻击防御的过程建模值得看看。
```
- 摘要  
提出了一种针对入侵防御用例的安全策略自动查找方法。将攻击者和防御者之间的互动建模为一个马尔可夫博弈，在没有人为干预的情况下，让攻击和防御策略通过强化学习和自演来进化  
![摘要](/assets/zhaiyao.png)
- 模型  
   将入侵防御建模为一个零和马尔可夫博弈(**Zero-sum Markov game**)，涉及攻击者代理和防御者代理  
   ![模型1](/assets/1model1.png)
   - 入侵防御博弈过程  
   上述图包括四个网络部分，$N_{start}$代表着攻击者，剩下的代表防御者设备，其中$N_{data}$是攻击者想要达到的部分即攻击者的目标组件。攻击者为破坏目标组件，攻击者通过侦察来探索基础设备，并在获取目标组件的路径上破坏组件；同时防御者进行监视，并增加防御，以防止攻击者访问目标组件。  
   【开始阶段】对于攻击者整个基于设施是个黑盒，相反，防御者完全了解网络的拓扑结构和网络漏洞(即每个节点有多种面对不同攻击的防御方法以及防御强度)，但无法观察攻击的状态。所描述的对抗过程是一款基于回合的游戏。
   - 入侵防御博弈模型  
   建立零和马尔可夫博弈模型，确定状态、动作以及奖励。  
   **状态**：每个节点$N_k$有与之相关的节点状态$S_k=\langle S^A_k,S^D_k\rangle$，其中防御状态$S^D_k$只对防御者可见，包括$m+1$的属性，前$m$个属性代表着防御者面对前$m$种攻击时防御的强度，以及一种特殊的属性即当前节点能检测出攻击的能力；同时防御状态$S^A_k$只对攻击者可见，包括$m$种的属性，各个属性分别代表着不同攻击类型的强度，例如拒绝服务攻击、跨域脚本攻击等。  
   **动作**：攻击者有两种动作，一种是侦察使防御状态$S^D_k$对攻击可见，一种是为某种攻击属性加1，如果节点的*某种攻击属性大于防御属性*，即认定攻击改节点有效，并且该节点的邻居节点对攻击者可见。同时如果攻击者采取了攻击动作但是达不到攻击要求，此时攻击可能被防御者以一定概率$p=\frac{S^D_{k,m+1}}{\omega+1}$检测到。防御者也有两种动作，一种监控动作，提高节点的检测能力，增加$S^D_{k,m+1}$值，另外一种是防御动作，提高某种防御能力的防御质量。  
   **奖励**：当攻击者破坏目标节点，攻击者获胜，或者当攻击者被检测到，防御者获胜。博弈双方的赢家获得+1的奖励，反之对手获得-1。
   - 入侵防御博弈场景    
   总共三个场景，场景1、2模型有较强的防御能力但是较弱的检测能力，在场景1种每个节点都包含着一个脆弱属性($S^D_{k,j} \le 1$)，同时场景2每个节点至少包含着一个脆弱属性；场景3中防御能力和检测能力都较弱。
   ![模型2](/assets/1model2.png)
- 方法
   - 自对弈方法学习策略  
   攻击者和防御者的动作从策略中采样，这些策略不是由专家定义，而是通过自我发挥的过程发展而来的。首先，随机策略初始化攻击者和防御者，接下来进行一系列的模拟，攻击者和防御者使用它们当前的策略相互竞争，并使用强化学习更新策略，直到两个策略充分收敛。  
   **但自对弈的策略不容易收敛，主要原因是环境是非平稳的，对于每个代理来说，当对手更新它的策略时，环境就会发生变化**  
   【解决方法】在训练期间从策略池中抽取对手策略，对手池增加了策略的多样性，从而减少了过度拟合，为了填充攻击防御池，在训练过程中定期将当前的攻击防范策略添加到攻击防御池中。
   ![模型3](/assets/1model3.png)
   - 强化学习算法  
   使用REINFORCE、PPO和PPO-AR(Autoregressive Policy Representation)算法，并且参数化策略，PPO-AR是在PPO的基础上做出了一定的改进：  
   *原本的状态空间为$|\mathcal{N}|\cdot(m+1)$简化为$|\mathcal{N}|+(m+1)$*  
   第一步选择哪个节点进行进攻或防御，第二步选择哪种攻击或防御方式。  
- 实验  
本文中已给出了源代码：<https://github.com/Limmen/gym-idsgame>
   - 学习对抗静态对手的策略  
   在对弈过程中，保持一方的策略固定，另一方学习它的策略对抗静态对手。研究了两个针对静态对手学习的案例，在第一种情况下，攻击者通过对抗一个称DEFENDMINIMAL的静态防御策略来学习自己的策略，用所有节点上最小的防御值更新属性；在第二种情况下，防御者通过对抗一个名为ATTACKMAXIMAL的静态攻击策略来学习自己的策略，用攻击者可见的所有节点上的最大攻击值更新属性。  
   **REINFORCE、PPO和PPO-AR方法：** 上部分显示训练攻击者对抗DEFENDMINIMAL，下部分显示训练防御者对抗ATTACKMAXIMAL
   ![实验1](/assets/1shiyan1.png)  
   PPO-AR超出基准算法，攻击者在场景1和场景2的胜率低于场景3，反映了场景1和场景2的初始防御较强，而场景3的初始防御较弱。同样，在图3最左边两列的相似曲线上，可以看到场景1和场景2有相似的防御。  
   检查学习到的攻击策略:我们发现学习到的攻击策略是确定性的，一般由两个步骤组成：(1)进行侦察，收集邻近节点的信息;(2)、利用识别出的漏洞。此外，学习到的防御策略也大多是确定性的，包括加固关键节点$N_{data}$和修补所有节点识别出的漏洞。最后，我们还发现防御者学会了利用可预测的攻击模式，并在攻击者可能下一次攻击的地方进行防御。  
   ![实验2](/assets/1shiyan2.png)  
   - 学习对抗动态对手的策略  
   研究攻击者和防御者同时学习的场景。策略有时在迭代中收敛，有时在迭代中振荡，并且发现这种行为依赖于特定的场景。场景2中，表示具有良好防御的基础设施，获得稳定的策略；相比之下，场景3捕获了具有较弱防御的基础设施，获得振荡的策略。
   ![实验3](/assets/1shiyan3.png)  
   PPO-AR在场景1、2之后策略收敛，然而，在场景3中，方法随着振幅的增加而振荡。振荡表明防御代理对对手策略的改变和超拟合做出了反应。尽管经验池池技术已经被引入来缓和振荡，但从结果中很明显，这在某些场景中是不够的。  
   **最终**，实现了将威胁识别与攻击预测结合起来的随机防御策略，以及将侦察与目标利用结合起来的随机攻击策略


***
**强化学习具有对策略进行广泛探索的特性，能找到对未知网络攻击的防御和恢复的最佳策略，促进网络安全方面的发展**

✔️ 网络安全研究小白，如有问题和建议，欢迎一起讨论🥺

